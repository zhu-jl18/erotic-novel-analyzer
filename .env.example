# 小说分析器配置文件
# 复制此文件为 .env 并修改以下配置
#
# 注意：所有 LLM 策略参数（温度/截断/retry/repair等）已迁移至 config/llm.yaml
# .env 只保留敏感信息和环境特定配置

# === 敏感信息（必填）===
# OpenAI 兼容 API 配置
API_BASE_URL=https://your-api.com/v1
API_KEY=sk-your-api-key
MODEL_NAME=gpt-4o

# === 环境特定（可选）===
# 无需配置小说目录；在网页中选择本地 .txt 文件即可导入分析。

# === 运行时配置（可选）===
# 服务器监听地址（默认仅本机）
# 127.0.0.1 = 仅本机访问（推荐）
# 0.0.0.0 = 局域网可访问（存在安全风险）
HOST=127.0.0.1

# 服务器端口
PORT=6103

# 日志级别：debug, info, warning, error, critical
LOG_LEVEL=warning

# === LLM 调试落盘（可选）===
# 开启后会把每次 LLM 调用的请求/响应 JSON 写入本地文件，便于排查 tool_call/schema 问题；
# 同时在接口报错时附带截断的原始响应片段（便于快速定位协议/字段问题）。
LLM_DUMP_ENABLED=false
LLM_DUMP_DIR=llm_dumps
# 截断保存，避免文件过大（按字符数）
LLM_DUMP_MAX_PROMPT_CHARS=12000
LLM_DUMP_MAX_RESPONSE_CHARS=30000

# === Repair 开关（可选，覆盖 config/llm.yaml）===
# 关闭后：当模型输出不符合 schema 时会直接报错，不再自动二次调用修复。
LLM_REPAIR_ENABLED=true
LLM_REPAIR_MAX_ATTEMPTS=1
